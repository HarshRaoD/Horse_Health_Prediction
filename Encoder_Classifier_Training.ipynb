{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Classifier Training\n",
    "Training a classifier using a pretrained autoencoder as a base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(988, 66)\n",
      "(247, 66)\n",
      "(988,)\n",
      "(247,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(\"data/Regular_processed/X_train.npy\")\n",
    "X_valid = np.load(\"data/Regular_processed/X_valid.npy\")\n",
    "y_train = np.load(\"data/Regular_processed/y_train.npy\")\n",
    "y_valid = np.load(\"data/Regular_processed/y_valid.npy\")\n",
    "\n",
    "for arr in [X_train, X_valid, y_train, y_valid]:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODELS = \"models/Encoder_Classifier\"\n",
    "MODEL_NAME = \"EnClass_66_18_6_ReLU_Sigmoid_10\"  #InputColumns_EmbeddingSize_HiddenSize_ActivationFunction(Encoder&Hidden)_ActivationFunctionFinal_Dropout%\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_columns=66, output_size=18, hidden_size=33, dropout_p=0.1, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_columns, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ffn(x)\n",
    "    \n",
    "class En_Classifier(nn.Module):\n",
    "    def __init__(self, input_columns=66, hidden_size_encoder=33, embedding_size = 18, hidden_size=6, output_size=3, dropout_p=0.1, encoder_checkpoint_path: str=None, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = Encoder(input_columns=input_columns, output_size=embedding_size, hidden_size=hidden_size_encoder, dropout_p=dropout_p)\n",
    "        if encoder_checkpoint_path is not None:\n",
    "            self.encoder.load_state_dict(torch.load(encoder_checkpoint_path))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        embeddings = self.encoder(x)\n",
    "        return self.classifier(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Horse_Health_Dataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        if x.shape[0] != y.shape[0]:\n",
    "            raise Exception(\"Dataset Error: Sizes of X and y dont match\")\n",
    "        \n",
    "        x_tensor = torch.from_numpy(x)\n",
    "        y_tensor = torch.from_numpy(y)\n",
    "        self.X = x_tensor.to(device)\n",
    "        self.y = y_tensor.to(device)\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "def create_dataloader(X, y, batch_size=128):\n",
    "    \"\"\"Returns a torch dataloader for the given dataset and batch_size\"\"\"\n",
    "    dataset = Horse_Health_Dataset(X, y)\n",
    "    dataloaders = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    \"\"\"Implements Early Stoppage of training when there is not progress in validation set\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, train_correct, train_f1 = 0, 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        y = y.type(torch.LongTensor).to(device)\n",
    "        X = X.type(torch.FloatTensor).to(device)\n",
    "        pred = model(X)\n",
    "        # print(f\"pred size = {str(pred.size())}\")\n",
    "        # print(f\"y size = {y.size()}\")\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (torch.argmax(pred, dim=1) == y).sum().item()\n",
    "        train_f1 += f1_score(y_pred=torch.argmax(pred, dim=1), y_true=y, average='micro')\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    train_correct /= size\n",
    "    train_f1 /= num_batches\n",
    "\n",
    "    return train_loss, train_correct, train_f1\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, test_correct, test_f1 = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            X = X.type(torch.FloatTensor).to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            test_correct += (torch.argmax(pred, dim=1) == y).sum().item()\n",
    "            test_f1 += f1_score(y_pred=torch.argmax(pred, dim=1), y_true=y, average='micro')\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    test_f1 /= num_batches\n",
    "    \n",
    "    return test_loss, test_correct, test_f1\n",
    "\n",
    "def train(train_dataloader: DataLoader, validation_dataloader: DataLoader, model: nn.Module, loss_fn, optimizer, epochs=100, patience=5):\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "    tr_loss, tr_accuracy, tr_f1 = [], [], []\n",
    "    va_loss, va_accuracy, va_f1 = [], [], []\n",
    "    for t in range(epochs):\n",
    "        train_loss, train_correct, train_f1 = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        valid_loss, valid_correct, valid_f1 = test_loop(validation_dataloader, model, loss_fn)\n",
    "\n",
    "        tr_loss.append(train_loss), tr_accuracy.append(train_correct), tr_f1.append(train_f1)\n",
    "        va_loss.append(valid_loss), va_accuracy.append(valid_correct), va_f1.append(valid_f1)\n",
    "\n",
    "        print(f\"Epoch {t+1}: Train_accuracy: {(100*train_correct):>0.2f}%, Train_loss: {train_loss:>8f} Train_F1_batchwise: {train_f1:>0.2f}, Validation_accuracy: {(100*valid_correct):>0.2f}%, Validation_loss: {valid_loss:>8f}, Validation_F1_batchwise :{valid_f1:>0.2f}\")\n",
    "\n",
    "        if (t + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"{PATH_TO_MODELS}/{MODEL_NAME}_epoch_{t+1}.pt\")\n",
    "\n",
    "        if early_stopper.early_stop(valid_loss):\n",
    "            print(\"Early Stopping Cutoff!\")\n",
    "            break\n",
    "\n",
    "    return tr_accuracy, tr_loss, tr_f1, va_accuracy, va_loss, va_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the architecture without the pretrained encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_accuracy: 34.72%, Train_loss: 1.088249 Train_F1_batchwise: 0.35, Validation_accuracy: 53.85%, Validation_loss: 1.008987, Validation_F1_batchwise :0.54\n",
      "Epoch 2: Train_accuracy: 45.45%, Train_loss: 1.013038 Train_F1_batchwise: 0.46, Validation_accuracy: 53.04%, Validation_loss: 0.941867, Validation_F1_batchwise :0.53\n",
      "Epoch 3: Train_accuracy: 56.98%, Train_loss: 0.952277 Train_F1_batchwise: 0.57, Validation_accuracy: 61.54%, Validation_loss: 0.900642, Validation_F1_batchwise :0.62\n",
      "Epoch 4: Train_accuracy: 62.55%, Train_loss: 0.914432 Train_F1_batchwise: 0.62, Validation_accuracy: 63.16%, Validation_loss: 0.888397, Validation_F1_batchwise :0.63\n",
      "Epoch 5: Train_accuracy: 66.70%, Train_loss: 0.885144 Train_F1_batchwise: 0.66, Validation_accuracy: 65.59%, Validation_loss: 0.882777, Validation_F1_batchwise :0.66\n",
      "Epoch 6: Train_accuracy: 67.51%, Train_loss: 0.868835 Train_F1_batchwise: 0.67, Validation_accuracy: 65.59%, Validation_loss: 0.856660, Validation_F1_batchwise :0.66\n",
      "Epoch 7: Train_accuracy: 68.22%, Train_loss: 0.850069 Train_F1_batchwise: 0.69, Validation_accuracy: 65.99%, Validation_loss: 0.862378, Validation_F1_batchwise :0.66\n",
      "Epoch 8: Train_accuracy: 68.32%, Train_loss: 0.844449 Train_F1_batchwise: 0.69, Validation_accuracy: 63.16%, Validation_loss: 0.880239, Validation_F1_batchwise :0.63\n",
      "Epoch 9: Train_accuracy: 70.14%, Train_loss: 0.838424 Train_F1_batchwise: 0.70, Validation_accuracy: 68.02%, Validation_loss: 0.861970, Validation_F1_batchwise :0.68\n",
      "Epoch 10: Train_accuracy: 70.65%, Train_loss: 0.839816 Train_F1_batchwise: 0.71, Validation_accuracy: 68.42%, Validation_loss: 0.860348, Validation_F1_batchwise :0.68\n",
      "Epoch 11: Train_accuracy: 72.27%, Train_loss: 0.826694 Train_F1_batchwise: 0.73, Validation_accuracy: 68.42%, Validation_loss: 0.857817, Validation_F1_batchwise :0.68\n",
      "Epoch 12: Train_accuracy: 72.37%, Train_loss: 0.819412 Train_F1_batchwise: 0.72, Validation_accuracy: 67.21%, Validation_loss: 0.868187, Validation_F1_batchwise :0.67\n",
      "Epoch 13: Train_accuracy: 73.18%, Train_loss: 0.815752 Train_F1_batchwise: 0.73, Validation_accuracy: 68.83%, Validation_loss: 0.846537, Validation_F1_batchwise :0.69\n",
      "Epoch 14: Train_accuracy: 73.58%, Train_loss: 0.810194 Train_F1_batchwise: 0.74, Validation_accuracy: 66.40%, Validation_loss: 0.863626, Validation_F1_batchwise :0.66\n",
      "Epoch 15: Train_accuracy: 73.28%, Train_loss: 0.819347 Train_F1_batchwise: 0.73, Validation_accuracy: 65.99%, Validation_loss: 0.876424, Validation_F1_batchwise :0.66\n",
      "Epoch 16: Train_accuracy: 72.87%, Train_loss: 0.819991 Train_F1_batchwise: 0.73, Validation_accuracy: 68.42%, Validation_loss: 0.860548, Validation_F1_batchwise :0.68\n",
      "Epoch 17: Train_accuracy: 73.99%, Train_loss: 0.812281 Train_F1_batchwise: 0.74, Validation_accuracy: 68.02%, Validation_loss: 0.853637, Validation_F1_batchwise :0.68\n",
      "Epoch 18: Train_accuracy: 75.10%, Train_loss: 0.802114 Train_F1_batchwise: 0.75, Validation_accuracy: 67.61%, Validation_loss: 0.855451, Validation_F1_batchwise :0.68\n",
      "Epoch 19: Train_accuracy: 74.90%, Train_loss: 0.804623 Train_F1_batchwise: 0.75, Validation_accuracy: 70.04%, Validation_loss: 0.833262, Validation_F1_batchwise :0.70\n",
      "Epoch 20: Train_accuracy: 75.40%, Train_loss: 0.799618 Train_F1_batchwise: 0.75, Validation_accuracy: 67.21%, Validation_loss: 0.863267, Validation_F1_batchwise :0.67\n",
      "Epoch 21: Train_accuracy: 75.71%, Train_loss: 0.797191 Train_F1_batchwise: 0.76, Validation_accuracy: 66.40%, Validation_loss: 0.867009, Validation_F1_batchwise :0.66\n",
      "Epoch 22: Train_accuracy: 77.02%, Train_loss: 0.780127 Train_F1_batchwise: 0.77, Validation_accuracy: 65.59%, Validation_loss: 0.872521, Validation_F1_batchwise :0.66\n",
      "Epoch 23: Train_accuracy: 75.81%, Train_loss: 0.788921 Train_F1_batchwise: 0.76, Validation_accuracy: 68.42%, Validation_loss: 0.860637, Validation_F1_batchwise :0.68\n",
      "Epoch 24: Train_accuracy: 75.71%, Train_loss: 0.790740 Train_F1_batchwise: 0.76, Validation_accuracy: 68.02%, Validation_loss: 0.855681, Validation_F1_batchwise :0.68\n",
      "Epoch 25: Train_accuracy: 77.13%, Train_loss: 0.779966 Train_F1_batchwise: 0.77, Validation_accuracy: 68.02%, Validation_loss: 0.860481, Validation_F1_batchwise :0.68\n",
      "Epoch 26: Train_accuracy: 76.82%, Train_loss: 0.789955 Train_F1_batchwise: 0.77, Validation_accuracy: 66.40%, Validation_loss: 0.878946, Validation_F1_batchwise :0.66\n",
      "Early Stopping Cutoff!\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloaders\n",
    "train_dataloader = create_dataloader(batch_size=128, X=X_train, y=y_train)\n",
    "valid_dataloader = create_dataloader(batch_size=128, X=X_valid, y=y_valid)\n",
    "\n",
    "# Create the model\n",
    "model = En_Classifier()\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Cannot use Cross Entropy Loss with Softmax\n",
    "\n",
    "# Train the model\n",
    "train_accuracy, train_loss, train_f1, valid_accuracy, valid_loss, valid_f1 = train(train_dataloader=train_dataloader, validation_dataloader=valid_dataloader,\n",
    "                                                                                   model=model, loss_fn=loss_fn, optimizer=optimizer, epochs=100, patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation F1 Score is: 0.6518218623481782\n"
     ]
    }
   ],
   "source": [
    "# Check final F1 Score\n",
    "final_valid_dataloader = create_dataloader(batch_size=len(X_valid), X=X_valid, y=y_valid)\n",
    "test_loss, test_correct, test_f1 = test_loop(final_valid_dataloader, model, loss_fn)\n",
    "print(f\"Final Validation F1 Score is: {test_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the pretrained encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_accuracy: 44.13%, Train_loss: 1.073614 Train_F1_batchwise: 0.44, Validation_accuracy: 54.66%, Validation_loss: 1.013204, Validation_F1_batchwise :0.55\n",
      "Epoch 2: Train_accuracy: 44.74%, Train_loss: 1.017569 Train_F1_batchwise: 0.45, Validation_accuracy: 57.89%, Validation_loss: 0.936946, Validation_F1_batchwise :0.58\n",
      "Epoch 3: Train_accuracy: 58.40%, Train_loss: 0.943994 Train_F1_batchwise: 0.58, Validation_accuracy: 66.80%, Validation_loss: 0.883458, Validation_F1_batchwise :0.67\n",
      "Epoch 4: Train_accuracy: 65.59%, Train_loss: 0.896909 Train_F1_batchwise: 0.66, Validation_accuracy: 64.78%, Validation_loss: 0.867224, Validation_F1_batchwise :0.65\n",
      "Epoch 5: Train_accuracy: 67.51%, Train_loss: 0.860222 Train_F1_batchwise: 0.68, Validation_accuracy: 66.80%, Validation_loss: 0.873647, Validation_F1_batchwise :0.67\n",
      "Epoch 6: Train_accuracy: 68.52%, Train_loss: 0.851586 Train_F1_batchwise: 0.68, Validation_accuracy: 68.02%, Validation_loss: 0.850407, Validation_F1_batchwise :0.68\n",
      "Epoch 7: Train_accuracy: 69.43%, Train_loss: 0.844085 Train_F1_batchwise: 0.69, Validation_accuracy: 68.02%, Validation_loss: 0.859136, Validation_F1_batchwise :0.68\n",
      "Epoch 8: Train_accuracy: 70.45%, Train_loss: 0.829450 Train_F1_batchwise: 0.70, Validation_accuracy: 71.26%, Validation_loss: 0.839430, Validation_F1_batchwise :0.71\n",
      "Epoch 9: Train_accuracy: 71.15%, Train_loss: 0.827427 Train_F1_batchwise: 0.71, Validation_accuracy: 68.83%, Validation_loss: 0.853731, Validation_F1_batchwise :0.69\n",
      "Epoch 10: Train_accuracy: 73.28%, Train_loss: 0.817481 Train_F1_batchwise: 0.73, Validation_accuracy: 68.42%, Validation_loss: 0.843391, Validation_F1_batchwise :0.68\n",
      "Epoch 11: Train_accuracy: 72.87%, Train_loss: 0.819732 Train_F1_batchwise: 0.73, Validation_accuracy: 68.83%, Validation_loss: 0.852441, Validation_F1_batchwise :0.69\n",
      "Epoch 12: Train_accuracy: 73.18%, Train_loss: 0.815073 Train_F1_batchwise: 0.73, Validation_accuracy: 68.42%, Validation_loss: 0.844134, Validation_F1_batchwise :0.69\n",
      "Epoch 13: Train_accuracy: 73.48%, Train_loss: 0.804775 Train_F1_batchwise: 0.74, Validation_accuracy: 66.40%, Validation_loss: 0.860799, Validation_F1_batchwise :0.67\n",
      "Epoch 14: Train_accuracy: 74.29%, Train_loss: 0.799960 Train_F1_batchwise: 0.74, Validation_accuracy: 68.02%, Validation_loss: 0.859645, Validation_F1_batchwise :0.68\n",
      "Epoch 15: Train_accuracy: 75.91%, Train_loss: 0.797338 Train_F1_batchwise: 0.76, Validation_accuracy: 68.42%, Validation_loss: 0.854838, Validation_F1_batchwise :0.68\n",
      "Early Stopping Cutoff!\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloaders\n",
    "train_dataloader = create_dataloader(batch_size=128, X=X_train, y=y_train)\n",
    "valid_dataloader = create_dataloader(batch_size=128, X=X_valid, y=y_valid)\n",
    "\n",
    "# Create the model\n",
    "model = En_Classifier(encoder_checkpoint_path=\"models/AutoEncoder/AutoEn_66_18_ReLU_10_justEncoder_epoch_45.pt\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Cannot use Cross Entropy Loss with Softmax\n",
    "\n",
    "# Train the model\n",
    "train_accuracy, train_loss, train_f1, valid_accuracy, valid_loss, valid_f1 = train(train_dataloader=train_dataloader, validation_dataloader=valid_dataloader,\n",
    "                                                                                   model=model, loss_fn=loss_fn, optimizer=optimizer, epochs=100, patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation F1 Score is: 0.6680161943319838\n"
     ]
    }
   ],
   "source": [
    "# Check final F1 Score\n",
    "final_valid_dataloader = create_dataloader(batch_size=len(X_valid), X=X_valid, y=y_valid)\n",
    "test_loss, test_correct, test_f1 = test_loop(final_valid_dataloader, model, loss_fn)\n",
    "print(f\"Final Validation F1 Score is: {test_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
