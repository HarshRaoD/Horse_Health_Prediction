{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Classifier Training\n",
    "Training a classifier using a pretrained autoencoder as a base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(988, 35)\n",
      "(247, 35)\n",
      "(988,)\n",
      "(247,)\n",
      "(824, 35)\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = \"data/Regular_Processed/With_Ordinal_Encoding/All_Positive/\"\n",
    "\n",
    "X_train = np.load(os.path.join(DIRECTORY, \"X_train.npy\"))\n",
    "X_valid = np.load(os.path.join(DIRECTORY, \"X_valid.npy\"))\n",
    "y_train = np.load(os.path.join(DIRECTORY, \"y_train.npy\"))\n",
    "y_valid = np.load(os.path.join(DIRECTORY, \"y_valid.npy\"))\n",
    "X_test = np.load(os.path.join(DIRECTORY, \"X_test.npy\"))\n",
    "\n",
    "for arr in [X_train, X_valid, y_train, y_valid, X_test]:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODELS = \"models/With_Ordinal_Encoding/All_Positive/\"\n",
    "MODEL_NAME = \"EnClass_35_12_6_ReLU_Sigmoid_10\"  #InputColumns_EmbeddingSize_HiddenSize_ActivationFunction(Encoder&Hidden)_ActivationFunctionFinal_Dropout%\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_columns=66, output_size=18, hidden_size=33, dropout_p=0.1, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_columns, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ffn(x)\n",
    "    \n",
    "class En_Classifier(nn.Module):\n",
    "    def __init__(self, input_columns=35, hidden_size_encoder=21, embedding_size = 12, hidden_size=6, output_size=3, dropout_p=0.1, encoder_checkpoint_path: str=None, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = Encoder(input_columns=input_columns, output_size=embedding_size, hidden_size=hidden_size_encoder, dropout_p=dropout_p)\n",
    "        if encoder_checkpoint_path is not None:\n",
    "            self.encoder.load_state_dict(torch.load(encoder_checkpoint_path))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        embeddings = self.encoder(x)\n",
    "        return self.classifier(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Horse_Health_Dataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        if x.shape[0] != y.shape[0]:\n",
    "            raise Exception(\"Dataset Error: Sizes of X and y dont match\")\n",
    "        \n",
    "        x_tensor = torch.from_numpy(x)\n",
    "        y_tensor = torch.from_numpy(y)\n",
    "        self.X = x_tensor.to(device)\n",
    "        self.y = y_tensor.to(device)\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "def create_dataloader(X, y, batch_size=128):\n",
    "    \"\"\"Returns a torch dataloader for the given dataset and batch_size\"\"\"\n",
    "    dataset = Horse_Health_Dataset(X, y)\n",
    "    dataloaders = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    \"\"\"Implements Early Stoppage of training when there is not progress in validation set\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, train_correct, train_f1 = 0, 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        y = y.type(torch.LongTensor).to(device)\n",
    "        X = X.type(torch.FloatTensor).to(device)\n",
    "        pred = model(X)\n",
    "        # print(f\"pred size = {str(pred.size())}\")\n",
    "        # print(f\"y size = {y.size()}\")\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (torch.argmax(pred, dim=1) == y).sum().item()\n",
    "        train_f1 += f1_score(y_pred=torch.argmax(pred, dim=1), y_true=y, average='micro')\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    train_correct /= size\n",
    "    train_f1 /= num_batches\n",
    "\n",
    "    return train_loss, train_correct, train_f1\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, test_correct, test_f1 = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            X = X.type(torch.FloatTensor).to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            test_correct += (torch.argmax(pred, dim=1) == y).sum().item()\n",
    "            test_f1 += f1_score(y_pred=torch.argmax(pred, dim=1), y_true=y, average='micro')\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    test_f1 /= num_batches\n",
    "    \n",
    "    return test_loss, test_correct, test_f1\n",
    "\n",
    "def train(train_dataloader: DataLoader, validation_dataloader: DataLoader, model: nn.Module, loss_fn, optimizer, epochs=100, patience=5):\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "    tr_loss, tr_accuracy, tr_f1 = [], [], []\n",
    "    va_loss, va_accuracy, va_f1 = [], [], []\n",
    "    for t in range(epochs):\n",
    "        train_loss, train_correct, train_f1 = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        valid_loss, valid_correct, valid_f1 = test_loop(validation_dataloader, model, loss_fn)\n",
    "\n",
    "        tr_loss.append(train_loss), tr_accuracy.append(train_correct), tr_f1.append(train_f1)\n",
    "        va_loss.append(valid_loss), va_accuracy.append(valid_correct), va_f1.append(valid_f1)\n",
    "\n",
    "        print(f\"Epoch {t+1}: Train_accuracy: {(100*train_correct):>0.2f}%, Train_loss: {train_loss:>8f} Train_F1_batchwise: {train_f1:>0.2f}, Validation_accuracy: {(100*valid_correct):>0.2f}%, Validation_loss: {valid_loss:>8f}, Validation_F1_batchwise :{valid_f1:>0.2f}\")\n",
    "\n",
    "        if (t + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"{PATH_TO_MODELS}/{MODEL_NAME}_epoch_{t+1}.pt\")\n",
    "\n",
    "        if early_stopper.early_stop(valid_loss):\n",
    "            print(\"Early Stopping Cutoff!\")\n",
    "            break\n",
    "\n",
    "    return tr_accuracy, tr_loss, tr_f1, va_accuracy, va_loss, va_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the architecture without the pretrained encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_accuracy: 30.06%, Train_loss: 1.097959 Train_F1_batchwise: 0.31, Validation_accuracy: 40.08%, Validation_loss: 1.078447, Validation_F1_batchwise :0.40\n",
      "Epoch 2: Train_accuracy: 47.27%, Train_loss: 1.034670 Train_F1_batchwise: 0.47, Validation_accuracy: 42.51%, Validation_loss: 1.063895, Validation_F1_batchwise :0.43\n",
      "Epoch 3: Train_accuracy: 48.99%, Train_loss: 1.008877 Train_F1_batchwise: 0.49, Validation_accuracy: 48.58%, Validation_loss: 1.014528, Validation_F1_batchwise :0.49\n",
      "Epoch 4: Train_accuracy: 55.77%, Train_loss: 0.954915 Train_F1_batchwise: 0.56, Validation_accuracy: 54.25%, Validation_loss: 0.965341, Validation_F1_batchwise :0.54\n",
      "Epoch 5: Train_accuracy: 58.40%, Train_loss: 0.930941 Train_F1_batchwise: 0.59, Validation_accuracy: 54.66%, Validation_loss: 0.961243, Validation_F1_batchwise :0.55\n",
      "Epoch 6: Train_accuracy: 61.03%, Train_loss: 0.912406 Train_F1_batchwise: 0.61, Validation_accuracy: 56.28%, Validation_loss: 0.951634, Validation_F1_batchwise :0.56\n",
      "Epoch 7: Train_accuracy: 62.55%, Train_loss: 0.900579 Train_F1_batchwise: 0.63, Validation_accuracy: 52.23%, Validation_loss: 0.964982, Validation_F1_batchwise :0.52\n",
      "Epoch 8: Train_accuracy: 63.66%, Train_loss: 0.893800 Train_F1_batchwise: 0.64, Validation_accuracy: 62.75%, Validation_loss: 0.924484, Validation_F1_batchwise :0.63\n",
      "Epoch 9: Train_accuracy: 64.78%, Train_loss: 0.885861 Train_F1_batchwise: 0.65, Validation_accuracy: 61.94%, Validation_loss: 0.915877, Validation_F1_batchwise :0.62\n",
      "Epoch 10: Train_accuracy: 65.49%, Train_loss: 0.864650 Train_F1_batchwise: 0.66, Validation_accuracy: 61.13%, Validation_loss: 0.920718, Validation_F1_batchwise :0.61\n",
      "Epoch 11: Train_accuracy: 65.08%, Train_loss: 0.879783 Train_F1_batchwise: 0.65, Validation_accuracy: 61.54%, Validation_loss: 0.900078, Validation_F1_batchwise :0.62\n",
      "Epoch 12: Train_accuracy: 66.60%, Train_loss: 0.859876 Train_F1_batchwise: 0.66, Validation_accuracy: 61.54%, Validation_loss: 0.918023, Validation_F1_batchwise :0.61\n",
      "Epoch 13: Train_accuracy: 64.98%, Train_loss: 0.867239 Train_F1_batchwise: 0.65, Validation_accuracy: 62.35%, Validation_loss: 0.916699, Validation_F1_batchwise :0.62\n",
      "Epoch 14: Train_accuracy: 65.69%, Train_loss: 0.857220 Train_F1_batchwise: 0.66, Validation_accuracy: 59.11%, Validation_loss: 0.936208, Validation_F1_batchwise :0.59\n",
      "Epoch 15: Train_accuracy: 65.99%, Train_loss: 0.861245 Train_F1_batchwise: 0.66, Validation_accuracy: 63.56%, Validation_loss: 0.900278, Validation_F1_batchwise :0.64\n",
      "Epoch 16: Train_accuracy: 67.81%, Train_loss: 0.846938 Train_F1_batchwise: 0.68, Validation_accuracy: 60.73%, Validation_loss: 0.912843, Validation_F1_batchwise :0.61\n",
      "Epoch 17: Train_accuracy: 66.30%, Train_loss: 0.850384 Train_F1_batchwise: 0.66, Validation_accuracy: 59.51%, Validation_loss: 0.923887, Validation_F1_batchwise :0.59\n",
      "Epoch 18: Train_accuracy: 66.30%, Train_loss: 0.846801 Train_F1_batchwise: 0.66, Validation_accuracy: 60.32%, Validation_loss: 0.904983, Validation_F1_batchwise :0.60\n",
      "Early Stopping Cutoff!\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloaders\n",
    "train_dataloader = create_dataloader(batch_size=128, X=X_train, y=y_train)\n",
    "valid_dataloader = create_dataloader(batch_size=128, X=X_valid, y=y_valid)\n",
    "\n",
    "# Create the model\n",
    "model = En_Classifier()\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Cannot use Cross Entropy Loss with Softmax\n",
    "\n",
    "# Train the model\n",
    "train_accuracy, train_loss, train_f1, valid_accuracy, valid_loss, valid_f1 = train(train_dataloader=train_dataloader, validation_dataloader=valid_dataloader,\n",
    "                                                                                   model=model, loss_fn=loss_fn, optimizer=optimizer, epochs=100, patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation F1 Score is: 0.6356275303643725\n"
     ]
    }
   ],
   "source": [
    "# Check final F1 Score\n",
    "final_valid_dataloader = create_dataloader(batch_size=len(X_valid), X=X_valid, y=y_valid)\n",
    "test_loss, test_correct, test_f1 = test_loop(final_valid_dataloader, model, loss_fn)\n",
    "print(f\"Final Validation F1 Score is: {test_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the pretrained encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_accuracy: 44.13%, Train_loss: 1.073614 Train_F1_batchwise: 0.44, Validation_accuracy: 54.66%, Validation_loss: 1.013204, Validation_F1_batchwise :0.55\n",
      "Epoch 2: Train_accuracy: 44.74%, Train_loss: 1.017569 Train_F1_batchwise: 0.45, Validation_accuracy: 57.89%, Validation_loss: 0.936946, Validation_F1_batchwise :0.58\n",
      "Epoch 3: Train_accuracy: 58.40%, Train_loss: 0.943994 Train_F1_batchwise: 0.58, Validation_accuracy: 66.80%, Validation_loss: 0.883458, Validation_F1_batchwise :0.67\n",
      "Epoch 4: Train_accuracy: 65.59%, Train_loss: 0.896909 Train_F1_batchwise: 0.66, Validation_accuracy: 64.78%, Validation_loss: 0.867224, Validation_F1_batchwise :0.65\n",
      "Epoch 5: Train_accuracy: 67.51%, Train_loss: 0.860222 Train_F1_batchwise: 0.68, Validation_accuracy: 66.80%, Validation_loss: 0.873647, Validation_F1_batchwise :0.67\n",
      "Epoch 6: Train_accuracy: 68.52%, Train_loss: 0.851586 Train_F1_batchwise: 0.68, Validation_accuracy: 68.02%, Validation_loss: 0.850407, Validation_F1_batchwise :0.68\n",
      "Epoch 7: Train_accuracy: 69.43%, Train_loss: 0.844085 Train_F1_batchwise: 0.69, Validation_accuracy: 68.02%, Validation_loss: 0.859136, Validation_F1_batchwise :0.68\n",
      "Epoch 8: Train_accuracy: 70.45%, Train_loss: 0.829450 Train_F1_batchwise: 0.70, Validation_accuracy: 71.26%, Validation_loss: 0.839430, Validation_F1_batchwise :0.71\n",
      "Epoch 9: Train_accuracy: 71.15%, Train_loss: 0.827427 Train_F1_batchwise: 0.71, Validation_accuracy: 68.83%, Validation_loss: 0.853731, Validation_F1_batchwise :0.69\n",
      "Epoch 10: Train_accuracy: 73.28%, Train_loss: 0.817481 Train_F1_batchwise: 0.73, Validation_accuracy: 68.42%, Validation_loss: 0.843391, Validation_F1_batchwise :0.68\n",
      "Epoch 11: Train_accuracy: 72.87%, Train_loss: 0.819732 Train_F1_batchwise: 0.73, Validation_accuracy: 68.83%, Validation_loss: 0.852441, Validation_F1_batchwise :0.69\n",
      "Epoch 12: Train_accuracy: 73.18%, Train_loss: 0.815073 Train_F1_batchwise: 0.73, Validation_accuracy: 68.42%, Validation_loss: 0.844134, Validation_F1_batchwise :0.69\n",
      "Epoch 13: Train_accuracy: 73.48%, Train_loss: 0.804775 Train_F1_batchwise: 0.74, Validation_accuracy: 66.40%, Validation_loss: 0.860799, Validation_F1_batchwise :0.67\n",
      "Epoch 14: Train_accuracy: 74.29%, Train_loss: 0.799960 Train_F1_batchwise: 0.74, Validation_accuracy: 68.02%, Validation_loss: 0.859645, Validation_F1_batchwise :0.68\n",
      "Epoch 15: Train_accuracy: 75.91%, Train_loss: 0.797338 Train_F1_batchwise: 0.76, Validation_accuracy: 68.42%, Validation_loss: 0.854838, Validation_F1_batchwise :0.68\n",
      "Early Stopping Cutoff!\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloaders\n",
    "train_dataloader = create_dataloader(batch_size=128, X=X_train, y=y_train)\n",
    "valid_dataloader = create_dataloader(batch_size=128, X=X_valid, y=y_valid)\n",
    "\n",
    "# Create the model\n",
    "model = En_Classifier(encoder_checkpoint_path=\"models/AutoEncoder/AutoEn_66_18_ReLU_10_justEncoder_epoch_45.pt\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Cannot use Cross Entropy Loss with Softmax\n",
    "\n",
    "# Train the model\n",
    "train_accuracy, train_loss, train_f1, valid_accuracy, valid_loss, valid_f1 = train(train_dataloader=train_dataloader, validation_dataloader=valid_dataloader,\n",
    "                                                                                   model=model, loss_fn=loss_fn, optimizer=optimizer, epochs=100, patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation F1 Score is: 0.6680161943319838\n"
     ]
    }
   ],
   "source": [
    "# Check final F1 Score\n",
    "final_valid_dataloader = create_dataloader(batch_size=len(X_valid), X=X_valid, y=y_valid)\n",
    "test_loss, test_correct, test_f1 = test_loop(final_valid_dataloader, model, loss_fn)\n",
    "print(f\"Final Validation F1 Score is: {test_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODELS = \"models/With_Ordinal_Encoding/All_Positive/\"\n",
    "MODEL_NAME = \"NNClass_35_12_6_Sigmoid_Sigmoid_10\"  #InputColumns_EmbeddingSize_HiddenSize_ActivationFunction(Encoder&Hidden)_ActivationFunctionFinal_Dropout%\n",
    "    \n",
    "class NN_Classifier(nn.Module):\n",
    "    def __init__(self, input_columns=35, embedding_size = 12, hidden_size=6, output_size=3, dropout_p=0.1, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_columns, embedding_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(embedding_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_accuracy: 35.43%, Train_loss: 1.096637 Train_F1_batchwise: 0.36, Validation_accuracy: 41.70%, Validation_loss: 1.088151, Validation_F1_batchwise :0.42\n",
      "Epoch 2: Train_accuracy: 47.17%, Train_loss: 1.068463 Train_F1_batchwise: 0.47, Validation_accuracy: 42.11%, Validation_loss: 1.078060, Validation_F1_batchwise :0.42\n",
      "Epoch 3: Train_accuracy: 47.57%, Train_loss: 1.053587 Train_F1_batchwise: 0.47, Validation_accuracy: 42.11%, Validation_loss: 1.077504, Validation_F1_batchwise :0.42\n",
      "Epoch 4: Train_accuracy: 47.57%, Train_loss: 1.041486 Train_F1_batchwise: 0.48, Validation_accuracy: 42.11%, Validation_loss: 1.075436, Validation_F1_batchwise :0.42\n",
      "Epoch 5: Train_accuracy: 47.57%, Train_loss: 1.036722 Train_F1_batchwise: 0.47, Validation_accuracy: 42.11%, Validation_loss: 1.075579, Validation_F1_batchwise :0.42\n",
      "Epoch 6: Train_accuracy: 47.57%, Train_loss: 1.030790 Train_F1_batchwise: 0.48, Validation_accuracy: 42.11%, Validation_loss: 1.064564, Validation_F1_batchwise :0.42\n",
      "Epoch 7: Train_accuracy: 47.57%, Train_loss: 1.022023 Train_F1_batchwise: 0.48, Validation_accuracy: 42.11%, Validation_loss: 1.065629, Validation_F1_batchwise :0.42\n",
      "Epoch 8: Train_accuracy: 47.57%, Train_loss: 1.019352 Train_F1_batchwise: 0.47, Validation_accuracy: 42.11%, Validation_loss: 1.059050, Validation_F1_batchwise :0.42\n",
      "Epoch 9: Train_accuracy: 47.57%, Train_loss: 1.011470 Train_F1_batchwise: 0.47, Validation_accuracy: 42.51%, Validation_loss: 1.050236, Validation_F1_batchwise :0.43\n",
      "Epoch 10: Train_accuracy: 49.39%, Train_loss: 1.003610 Train_F1_batchwise: 0.49, Validation_accuracy: 44.53%, Validation_loss: 1.043154, Validation_F1_batchwise :0.45\n",
      "Epoch 11: Train_accuracy: 54.66%, Train_loss: 0.995549 Train_F1_batchwise: 0.55, Validation_accuracy: 53.85%, Validation_loss: 1.031189, Validation_F1_batchwise :0.54\n",
      "Epoch 12: Train_accuracy: 59.21%, Train_loss: 0.978572 Train_F1_batchwise: 0.59, Validation_accuracy: 52.63%, Validation_loss: 1.024359, Validation_F1_batchwise :0.52\n",
      "Epoch 13: Train_accuracy: 60.32%, Train_loss: 0.969879 Train_F1_batchwise: 0.61, Validation_accuracy: 54.66%, Validation_loss: 1.012423, Validation_F1_batchwise :0.55\n",
      "Epoch 14: Train_accuracy: 60.12%, Train_loss: 0.960566 Train_F1_batchwise: 0.60, Validation_accuracy: 53.85%, Validation_loss: 1.007938, Validation_F1_batchwise :0.54\n",
      "Epoch 15: Train_accuracy: 60.53%, Train_loss: 0.956013 Train_F1_batchwise: 0.61, Validation_accuracy: 53.85%, Validation_loss: 1.007391, Validation_F1_batchwise :0.54\n",
      "Epoch 16: Train_accuracy: 60.22%, Train_loss: 0.947633 Train_F1_batchwise: 0.61, Validation_accuracy: 52.63%, Validation_loss: 1.008654, Validation_F1_batchwise :0.53\n",
      "Epoch 17: Train_accuracy: 62.04%, Train_loss: 0.937121 Train_F1_batchwise: 0.62, Validation_accuracy: 52.63%, Validation_loss: 0.996923, Validation_F1_batchwise :0.53\n",
      "Epoch 18: Train_accuracy: 61.44%, Train_loss: 0.935720 Train_F1_batchwise: 0.61, Validation_accuracy: 53.85%, Validation_loss: 0.993962, Validation_F1_batchwise :0.54\n",
      "Epoch 19: Train_accuracy: 61.74%, Train_loss: 0.931342 Train_F1_batchwise: 0.62, Validation_accuracy: 55.47%, Validation_loss: 0.987627, Validation_F1_batchwise :0.56\n",
      "Epoch 20: Train_accuracy: 62.65%, Train_loss: 0.920247 Train_F1_batchwise: 0.63, Validation_accuracy: 53.85%, Validation_loss: 0.993174, Validation_F1_batchwise :0.54\n",
      "Epoch 21: Train_accuracy: 61.84%, Train_loss: 0.924239 Train_F1_batchwise: 0.62, Validation_accuracy: 52.63%, Validation_loss: 0.988448, Validation_F1_batchwise :0.53\n",
      "Epoch 22: Train_accuracy: 62.65%, Train_loss: 0.920585 Train_F1_batchwise: 0.63, Validation_accuracy: 55.47%, Validation_loss: 0.976272, Validation_F1_batchwise :0.55\n",
      "Epoch 23: Train_accuracy: 62.65%, Train_loss: 0.914065 Train_F1_batchwise: 0.62, Validation_accuracy: 54.66%, Validation_loss: 0.978273, Validation_F1_batchwise :0.55\n",
      "Epoch 24: Train_accuracy: 64.37%, Train_loss: 0.905645 Train_F1_batchwise: 0.64, Validation_accuracy: 53.04%, Validation_loss: 0.980640, Validation_F1_batchwise :0.53\n",
      "Epoch 25: Train_accuracy: 64.37%, Train_loss: 0.904522 Train_F1_batchwise: 0.65, Validation_accuracy: 53.85%, Validation_loss: 0.974378, Validation_F1_batchwise :0.54\n",
      "Epoch 26: Train_accuracy: 63.56%, Train_loss: 0.908366 Train_F1_batchwise: 0.63, Validation_accuracy: 53.85%, Validation_loss: 0.966877, Validation_F1_batchwise :0.54\n",
      "Epoch 27: Train_accuracy: 64.37%, Train_loss: 0.900624 Train_F1_batchwise: 0.64, Validation_accuracy: 53.85%, Validation_loss: 0.976908, Validation_F1_batchwise :0.54\n",
      "Epoch 28: Train_accuracy: 64.57%, Train_loss: 0.895378 Train_F1_batchwise: 0.64, Validation_accuracy: 52.23%, Validation_loss: 0.969811, Validation_F1_batchwise :0.52\n",
      "Epoch 29: Train_accuracy: 63.06%, Train_loss: 0.900545 Train_F1_batchwise: 0.63, Validation_accuracy: 52.63%, Validation_loss: 0.965198, Validation_F1_batchwise :0.53\n",
      "Epoch 30: Train_accuracy: 63.87%, Train_loss: 0.894058 Train_F1_batchwise: 0.64, Validation_accuracy: 53.44%, Validation_loss: 0.960529, Validation_F1_batchwise :0.54\n",
      "Epoch 31: Train_accuracy: 64.37%, Train_loss: 0.887007 Train_F1_batchwise: 0.64, Validation_accuracy: 52.23%, Validation_loss: 0.971898, Validation_F1_batchwise :0.52\n",
      "Epoch 32: Train_accuracy: 65.38%, Train_loss: 0.881723 Train_F1_batchwise: 0.65, Validation_accuracy: 51.82%, Validation_loss: 0.968608, Validation_F1_batchwise :0.52\n",
      "Epoch 33: Train_accuracy: 64.98%, Train_loss: 0.880399 Train_F1_batchwise: 0.65, Validation_accuracy: 52.63%, Validation_loss: 0.964757, Validation_F1_batchwise :0.53\n",
      "Epoch 34: Train_accuracy: 65.99%, Train_loss: 0.875785 Train_F1_batchwise: 0.66, Validation_accuracy: 52.63%, Validation_loss: 0.956776, Validation_F1_batchwise :0.53\n",
      "Epoch 35: Train_accuracy: 65.49%, Train_loss: 0.874261 Train_F1_batchwise: 0.66, Validation_accuracy: 51.82%, Validation_loss: 0.958465, Validation_F1_batchwise :0.52\n",
      "Epoch 36: Train_accuracy: 65.28%, Train_loss: 0.872699 Train_F1_batchwise: 0.65, Validation_accuracy: 53.04%, Validation_loss: 0.946128, Validation_F1_batchwise :0.53\n",
      "Epoch 37: Train_accuracy: 64.78%, Train_loss: 0.879291 Train_F1_batchwise: 0.65, Validation_accuracy: 51.82%, Validation_loss: 0.957130, Validation_F1_batchwise :0.52\n",
      "Epoch 38: Train_accuracy: 65.08%, Train_loss: 0.868856 Train_F1_batchwise: 0.65, Validation_accuracy: 52.63%, Validation_loss: 0.958743, Validation_F1_batchwise :0.53\n",
      "Epoch 39: Train_accuracy: 65.18%, Train_loss: 0.868328 Train_F1_batchwise: 0.65, Validation_accuracy: 51.82%, Validation_loss: 0.950606, Validation_F1_batchwise :0.52\n",
      "Epoch 40: Train_accuracy: 65.49%, Train_loss: 0.870188 Train_F1_batchwise: 0.65, Validation_accuracy: 53.04%, Validation_loss: 0.945161, Validation_F1_batchwise :0.53\n",
      "Epoch 41: Train_accuracy: 65.69%, Train_loss: 0.866599 Train_F1_batchwise: 0.66, Validation_accuracy: 54.66%, Validation_loss: 0.939186, Validation_F1_batchwise :0.55\n",
      "Epoch 42: Train_accuracy: 67.11%, Train_loss: 0.859954 Train_F1_batchwise: 0.67, Validation_accuracy: 52.23%, Validation_loss: 0.945115, Validation_F1_batchwise :0.52\n",
      "Epoch 43: Train_accuracy: 65.49%, Train_loss: 0.862620 Train_F1_batchwise: 0.66, Validation_accuracy: 53.85%, Validation_loss: 0.945898, Validation_F1_batchwise :0.54\n",
      "Epoch 44: Train_accuracy: 67.31%, Train_loss: 0.858470 Train_F1_batchwise: 0.68, Validation_accuracy: 52.23%, Validation_loss: 0.949926, Validation_F1_batchwise :0.52\n",
      "Epoch 45: Train_accuracy: 64.88%, Train_loss: 0.862951 Train_F1_batchwise: 0.65, Validation_accuracy: 55.06%, Validation_loss: 0.937584, Validation_F1_batchwise :0.55\n",
      "Epoch 46: Train_accuracy: 66.30%, Train_loss: 0.858562 Train_F1_batchwise: 0.66, Validation_accuracy: 53.04%, Validation_loss: 0.945814, Validation_F1_batchwise :0.53\n",
      "Epoch 47: Train_accuracy: 66.80%, Train_loss: 0.855160 Train_F1_batchwise: 0.67, Validation_accuracy: 50.61%, Validation_loss: 0.949379, Validation_F1_batchwise :0.51\n",
      "Epoch 48: Train_accuracy: 66.40%, Train_loss: 0.853223 Train_F1_batchwise: 0.66, Validation_accuracy: 54.25%, Validation_loss: 0.937152, Validation_F1_batchwise :0.54\n",
      "Epoch 49: Train_accuracy: 67.11%, Train_loss: 0.852544 Train_F1_batchwise: 0.67, Validation_accuracy: 53.44%, Validation_loss: 0.945962, Validation_F1_batchwise :0.53\n",
      "Epoch 50: Train_accuracy: 67.81%, Train_loss: 0.852862 Train_F1_batchwise: 0.68, Validation_accuracy: 57.89%, Validation_loss: 0.932397, Validation_F1_batchwise :0.58\n",
      "Epoch 51: Train_accuracy: 67.71%, Train_loss: 0.855214 Train_F1_batchwise: 0.68, Validation_accuracy: 57.49%, Validation_loss: 0.941112, Validation_F1_batchwise :0.57\n",
      "Epoch 52: Train_accuracy: 68.62%, Train_loss: 0.855456 Train_F1_batchwise: 0.69, Validation_accuracy: 60.32%, Validation_loss: 0.944412, Validation_F1_batchwise :0.60\n",
      "Epoch 53: Train_accuracy: 68.12%, Train_loss: 0.857043 Train_F1_batchwise: 0.68, Validation_accuracy: 59.11%, Validation_loss: 0.931231, Validation_F1_batchwise :0.59\n",
      "Epoch 54: Train_accuracy: 69.74%, Train_loss: 0.853991 Train_F1_batchwise: 0.70, Validation_accuracy: 63.97%, Validation_loss: 0.919239, Validation_F1_batchwise :0.64\n",
      "Epoch 55: Train_accuracy: 69.64%, Train_loss: 0.846061 Train_F1_batchwise: 0.70, Validation_accuracy: 59.92%, Validation_loss: 0.944273, Validation_F1_batchwise :0.60\n",
      "Epoch 56: Train_accuracy: 70.24%, Train_loss: 0.846463 Train_F1_batchwise: 0.70, Validation_accuracy: 61.13%, Validation_loss: 0.938493, Validation_F1_batchwise :0.61\n",
      "Epoch 57: Train_accuracy: 70.85%, Train_loss: 0.848511 Train_F1_batchwise: 0.71, Validation_accuracy: 61.54%, Validation_loss: 0.944369, Validation_F1_batchwise :0.62\n",
      "Epoch 58: Train_accuracy: 69.94%, Train_loss: 0.847284 Train_F1_batchwise: 0.70, Validation_accuracy: 60.32%, Validation_loss: 0.938749, Validation_F1_batchwise :0.60\n",
      "Epoch 59: Train_accuracy: 70.75%, Train_loss: 0.845282 Train_F1_batchwise: 0.71, Validation_accuracy: 60.32%, Validation_loss: 0.941066, Validation_F1_batchwise :0.61\n",
      "Epoch 60: Train_accuracy: 69.74%, Train_loss: 0.846075 Train_F1_batchwise: 0.70, Validation_accuracy: 59.11%, Validation_loss: 0.941038, Validation_F1_batchwise :0.59\n",
      "Epoch 61: Train_accuracy: 70.04%, Train_loss: 0.846521 Train_F1_batchwise: 0.70, Validation_accuracy: 62.75%, Validation_loss: 0.934933, Validation_F1_batchwise :0.63\n",
      "Early Stopping Cutoff!\n",
      "Final Validation F1 Score is: 0.6356275303643725\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloaders\n",
    "train_dataloader = create_dataloader(batch_size=128, X=X_train, y=y_train)\n",
    "valid_dataloader = create_dataloader(batch_size=128, X=X_valid, y=y_valid)\n",
    "\n",
    "# Create the model\n",
    "model = NN_Classifier()\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Cannot use Cross Entropy Loss with Softmax\n",
    "\n",
    "# Train the model\n",
    "train_accuracy, train_loss, train_f1, valid_accuracy, valid_loss, valid_f1 = train(train_dataloader=train_dataloader, validation_dataloader=valid_dataloader,\n",
    "                                                                                   model=model, loss_fn=loss_fn, optimizer=optimizer, epochs=100, patience=7)\n",
    "\n",
    "# Check final F1 Score\n",
    "final_valid_dataloader = create_dataloader(batch_size=len(X_valid), X=X_valid, y=y_valid)\n",
    "test_loss, test_correct, test_f1 = test_loop(final_valid_dataloader, model, loss_fn)\n",
    "print(f\"Final Validation F1 Score is: {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = create_dataloader(batch_size=128, X=X_test, y=y_valid)\n",
    "test_x_tensor = torch.from_numpy(X_test)\n",
    "test_x_tensor = test_x_tensor.type(torch.FloatTensor).to(device)\n",
    "pred = model(test_x_tensor)\n",
    "y_pred = torch.argmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rt/yf3df5s52_589vft9vpfml9c0000gn/T/ipykernel_96894/2488446883.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PRED_CATEGORIES = ['died', 'euthanized', 'lived']\n",
    "pred_list = [PRED_CATEGORIES[el] for el in y_pred.tolist()]\n",
    "\n",
    "submission_df = pd.DataFrame()\n",
    "df_test_raw = pd.read_csv(\"data/test.csv\")\n",
    "submission_df['id'] = df_test_raw['id']\n",
    "submission_df['outcome'] = pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"submissions/With_Ordinal_Encoding/All_Positive/\"\n",
    "submission_df.to_csv(os.path.join(DIR, \"NN_35_12_6_Sigmoid_Sigmoid_epoch_60.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
