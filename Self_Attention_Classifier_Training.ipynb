{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention Classifier Training\n",
    "Training a classifier using a pretrained autoencoder as a base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(988, 66)\n",
      "(247, 66)\n",
      "(988,)\n",
      "(247,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(\"data/Regular_processed/X_train.npy\")\n",
    "X_valid = np.load(\"data/Regular_processed/X_valid.npy\")\n",
    "y_train = np.load(\"data/Regular_processed/y_train.npy\")\n",
    "y_valid = np.load(\"data/Regular_processed/y_valid.npy\")\n",
    "\n",
    "for arr in [X_train, X_valid, y_train, y_valid]:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODELS = \"models/Self_Attention_Classifier\"\n",
    "MODEL_NAME = \"AtClass_66_24_8_ReLU_Sigmoid_10\"  #InputColumns_HiddenSize0_HiddenSize1_ActivationFunction(Encoder&Hidden)_ActivationFunctionFinal_Dropout%\n",
    "    \n",
    "class Attention_Classifier(nn.Module):\n",
    "    def __init__(self, input_columns=66, hidden_size=[24, 8], output_size=3, dropout_p=0.1, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=1, num_heads=1, batch_first=True, dropout=dropout_p)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_columns, hidden_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size[1], output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_unsqueeze = torch.unsqueeze(x, dim=2)\n",
    "        self_att_output, _ = self.attention(x_unsqueeze, x_unsqueeze, x_unsqueeze)\n",
    "        self_att_output_squeezed = torch.squeeze(self_att_output)\n",
    "        return self.classifier(self_att_output_squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Horse_Health_Dataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        if x.shape[0] != y.shape[0]:\n",
    "            raise Exception(\"Dataset Error: Sizes of X and y dont match\")\n",
    "        \n",
    "        x_tensor = torch.from_numpy(x)\n",
    "        y_tensor = torch.from_numpy(y)\n",
    "        self.X = x_tensor.to(device)\n",
    "        self.y = y_tensor.to(device)\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "def create_dataloader(X, y, batch_size=128):\n",
    "    \"\"\"Returns a torch dataloader for the given dataset and batch_size\"\"\"\n",
    "    dataset = Horse_Health_Dataset(X, y)\n",
    "    dataloaders = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    \"\"\"Implements Early Stoppage of training when there is not progress in validation set\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, train_correct, train_f1 = 0, 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        y = y.type(torch.LongTensor).to(device)\n",
    "        X = X.type(torch.FloatTensor).to(device)\n",
    "        pred = model(X)\n",
    "        # print(f\"pred size = {str(pred.size())}\")\n",
    "        # print(f\"y size = {y.size()}\")\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (torch.argmax(pred, dim=1) == y).sum().item()\n",
    "        train_f1 += f1_score(y_pred=torch.argmax(pred, dim=1), y_true=y, average='micro')\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    train_correct /= size\n",
    "    train_f1 /= num_batches\n",
    "\n",
    "    return train_loss, train_correct, train_f1\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, test_correct, test_f1 = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            X = X.type(torch.FloatTensor).to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            test_correct += (torch.argmax(pred, dim=1) == y).sum().item()\n",
    "            test_f1 += f1_score(y_pred=torch.argmax(pred, dim=1), y_true=y, average='micro')\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    test_f1 /= num_batches\n",
    "    \n",
    "    return test_loss, test_correct, test_f1\n",
    "\n",
    "def train(train_dataloader: DataLoader, validation_dataloader: DataLoader, model: nn.Module, loss_fn, optimizer, epochs=100, patience=5):\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "    tr_loss, tr_accuracy, tr_f1 = [], [], []\n",
    "    va_loss, va_accuracy, va_f1 = [], [], []\n",
    "    for t in range(epochs):\n",
    "        train_loss, train_correct, train_f1 = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        valid_loss, valid_correct, valid_f1 = test_loop(validation_dataloader, model, loss_fn)\n",
    "\n",
    "        tr_loss.append(train_loss), tr_accuracy.append(train_correct), tr_f1.append(train_f1)\n",
    "        va_loss.append(valid_loss), va_accuracy.append(valid_correct), va_f1.append(valid_f1)\n",
    "\n",
    "        print(f\"Epoch {t+1}: Train_accuracy: {(100*train_correct):>0.2f}%, Train_loss: {train_loss:>8f} Train_F1_batchwise: {train_f1:>0.2f}, Validation_accuracy: {(100*valid_correct):>0.2f}%, Validation_loss: {valid_loss:>8f}, Validation_F1_batchwise :{valid_f1:>0.2f}\")\n",
    "\n",
    "        if (t + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"{PATH_TO_MODELS}/{MODEL_NAME}_epoch_{t+1}.pt\")\n",
    "\n",
    "        if early_stopper.early_stop(valid_loss):\n",
    "            print(\"Early Stopping Cutoff!\")\n",
    "            break\n",
    "\n",
    "    return tr_accuracy, tr_loss, tr_f1, va_accuracy, va_loss, va_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_accuracy: 34.21%, Train_loss: 1.091768 Train_F1_batchwise: 0.35, Validation_accuracy: 48.99%, Validation_loss: 1.059820, Validation_F1_batchwise :0.49\n",
      "Epoch 2: Train_accuracy: 45.85%, Train_loss: 1.056583 Train_F1_batchwise: 0.46, Validation_accuracy: 48.58%, Validation_loss: 1.039802, Validation_F1_batchwise :0.49\n",
      "Epoch 3: Train_accuracy: 45.85%, Train_loss: 1.051872 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.043078, Validation_F1_batchwise :0.49\n",
      "Epoch 4: Train_accuracy: 45.34%, Train_loss: 1.055975 Train_F1_batchwise: 0.45, Validation_accuracy: 48.58%, Validation_loss: 1.039779, Validation_F1_batchwise :0.49\n",
      "Epoch 5: Train_accuracy: 45.55%, Train_loss: 1.050688 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.033206, Validation_F1_batchwise :0.49\n",
      "Epoch 6: Train_accuracy: 45.85%, Train_loss: 1.048824 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.037905, Validation_F1_batchwise :0.49\n",
      "Epoch 7: Train_accuracy: 45.85%, Train_loss: 1.049919 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.035312, Validation_F1_batchwise :0.49\n",
      "Epoch 8: Train_accuracy: 45.85%, Train_loss: 1.048848 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.040791, Validation_F1_batchwise :0.49\n",
      "Epoch 9: Train_accuracy: 45.85%, Train_loss: 1.045560 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.037050, Validation_F1_batchwise :0.49\n",
      "Epoch 10: Train_accuracy: 45.85%, Train_loss: 1.047721 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.040706, Validation_F1_batchwise :0.49\n",
      "Epoch 11: Train_accuracy: 45.85%, Train_loss: 1.051541 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.039211, Validation_F1_batchwise :0.49\n",
      "Epoch 12: Train_accuracy: 45.85%, Train_loss: 1.050109 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.036182, Validation_F1_batchwise :0.49\n",
      "Epoch 13: Train_accuracy: 45.85%, Train_loss: 1.048407 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.040585, Validation_F1_batchwise :0.49\n",
      "Epoch 14: Train_accuracy: 45.85%, Train_loss: 1.044830 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 1.029823, Validation_F1_batchwise :0.49\n",
      "Epoch 15: Train_accuracy: 45.85%, Train_loss: 1.041723 Train_F1_batchwise: 0.45, Validation_accuracy: 48.99%, Validation_loss: 1.022139, Validation_F1_batchwise :0.49\n",
      "Epoch 16: Train_accuracy: 46.05%, Train_loss: 1.011125 Train_F1_batchwise: 0.46, Validation_accuracy: 48.99%, Validation_loss: 0.982909, Validation_F1_batchwise :0.49\n",
      "Epoch 17: Train_accuracy: 48.79%, Train_loss: 0.976966 Train_F1_batchwise: 0.49, Validation_accuracy: 53.85%, Validation_loss: 0.944269, Validation_F1_batchwise :0.54\n",
      "Epoch 18: Train_accuracy: 51.01%, Train_loss: 0.930190 Train_F1_batchwise: 0.51, Validation_accuracy: 51.82%, Validation_loss: 0.905095, Validation_F1_batchwise :0.52\n",
      "Epoch 19: Train_accuracy: 58.20%, Train_loss: 0.916013 Train_F1_batchwise: 0.59, Validation_accuracy: 62.75%, Validation_loss: 0.910660, Validation_F1_batchwise :0.63\n",
      "Epoch 20: Train_accuracy: 65.79%, Train_loss: 0.895647 Train_F1_batchwise: 0.66, Validation_accuracy: 60.73%, Validation_loss: 0.894356, Validation_F1_batchwise :0.61\n",
      "Epoch 21: Train_accuracy: 66.70%, Train_loss: 0.872858 Train_F1_batchwise: 0.67, Validation_accuracy: 67.61%, Validation_loss: 0.878586, Validation_F1_batchwise :0.67\n",
      "Epoch 22: Train_accuracy: 67.31%, Train_loss: 0.866526 Train_F1_batchwise: 0.67, Validation_accuracy: 65.18%, Validation_loss: 0.859900, Validation_F1_batchwise :0.65\n",
      "Epoch 23: Train_accuracy: 69.94%, Train_loss: 0.846099 Train_F1_batchwise: 0.70, Validation_accuracy: 63.16%, Validation_loss: 0.881974, Validation_F1_batchwise :0.63\n",
      "Epoch 24: Train_accuracy: 68.93%, Train_loss: 0.837981 Train_F1_batchwise: 0.69, Validation_accuracy: 65.99%, Validation_loss: 0.868292, Validation_F1_batchwise :0.66\n",
      "Epoch 25: Train_accuracy: 70.45%, Train_loss: 0.835299 Train_F1_batchwise: 0.71, Validation_accuracy: 63.56%, Validation_loss: 0.876992, Validation_F1_batchwise :0.63\n",
      "Epoch 26: Train_accuracy: 71.96%, Train_loss: 0.827888 Train_F1_batchwise: 0.72, Validation_accuracy: 66.40%, Validation_loss: 0.862870, Validation_F1_batchwise :0.66\n",
      "Epoch 27: Train_accuracy: 72.47%, Train_loss: 0.820801 Train_F1_batchwise: 0.73, Validation_accuracy: 66.40%, Validation_loss: 0.868639, Validation_F1_batchwise :0.66\n",
      "Epoch 28: Train_accuracy: 72.17%, Train_loss: 0.826253 Train_F1_batchwise: 0.72, Validation_accuracy: 66.80%, Validation_loss: 0.878480, Validation_F1_batchwise :0.67\n",
      "Epoch 29: Train_accuracy: 73.28%, Train_loss: 0.812743 Train_F1_batchwise: 0.73, Validation_accuracy: 68.02%, Validation_loss: 0.857300, Validation_F1_batchwise :0.68\n",
      "Epoch 30: Train_accuracy: 74.09%, Train_loss: 0.807304 Train_F1_batchwise: 0.74, Validation_accuracy: 66.40%, Validation_loss: 0.866324, Validation_F1_batchwise :0.66\n",
      "Epoch 31: Train_accuracy: 73.79%, Train_loss: 0.804504 Train_F1_batchwise: 0.74, Validation_accuracy: 66.80%, Validation_loss: 0.870528, Validation_F1_batchwise :0.67\n",
      "Epoch 32: Train_accuracy: 72.17%, Train_loss: 0.809387 Train_F1_batchwise: 0.72, Validation_accuracy: 64.37%, Validation_loss: 0.893138, Validation_F1_batchwise :0.64\n",
      "Epoch 33: Train_accuracy: 72.67%, Train_loss: 0.810628 Train_F1_batchwise: 0.73, Validation_accuracy: 65.99%, Validation_loss: 0.871098, Validation_F1_batchwise :0.66\n",
      "Epoch 34: Train_accuracy: 72.37%, Train_loss: 0.814042 Train_F1_batchwise: 0.72, Validation_accuracy: 67.21%, Validation_loss: 0.887194, Validation_F1_batchwise :0.67\n",
      "Epoch 35: Train_accuracy: 72.37%, Train_loss: 0.814102 Train_F1_batchwise: 0.72, Validation_accuracy: 63.16%, Validation_loss: 0.901668, Validation_F1_batchwise :0.63\n",
      "Epoch 36: Train_accuracy: 74.60%, Train_loss: 0.798497 Train_F1_batchwise: 0.75, Validation_accuracy: 62.75%, Validation_loss: 0.888030, Validation_F1_batchwise :0.63\n",
      "Epoch 37: Train_accuracy: 73.18%, Train_loss: 0.807720 Train_F1_batchwise: 0.73, Validation_accuracy: 63.97%, Validation_loss: 0.891334, Validation_F1_batchwise :0.64\n",
      "Epoch 38: Train_accuracy: 72.67%, Train_loss: 0.813122 Train_F1_batchwise: 0.73, Validation_accuracy: 64.37%, Validation_loss: 0.870979, Validation_F1_batchwise :0.65\n",
      "Epoch 39: Train_accuracy: 73.68%, Train_loss: 0.793386 Train_F1_batchwise: 0.74, Validation_accuracy: 68.83%, Validation_loss: 0.845362, Validation_F1_batchwise :0.69\n",
      "Epoch 40: Train_accuracy: 75.91%, Train_loss: 0.790749 Train_F1_batchwise: 0.76, Validation_accuracy: 68.02%, Validation_loss: 0.849312, Validation_F1_batchwise :0.68\n",
      "Epoch 41: Train_accuracy: 74.80%, Train_loss: 0.791598 Train_F1_batchwise: 0.75, Validation_accuracy: 67.61%, Validation_loss: 0.856376, Validation_F1_batchwise :0.68\n",
      "Epoch 42: Train_accuracy: 75.20%, Train_loss: 0.794114 Train_F1_batchwise: 0.75, Validation_accuracy: 64.37%, Validation_loss: 0.883499, Validation_F1_batchwise :0.64\n",
      "Epoch 43: Train_accuracy: 77.02%, Train_loss: 0.774400 Train_F1_batchwise: 0.77, Validation_accuracy: 66.40%, Validation_loss: 0.867683, Validation_F1_batchwise :0.66\n",
      "Epoch 44: Train_accuracy: 75.81%, Train_loss: 0.781334 Train_F1_batchwise: 0.76, Validation_accuracy: 66.40%, Validation_loss: 0.870571, Validation_F1_batchwise :0.66\n",
      "Epoch 45: Train_accuracy: 77.43%, Train_loss: 0.775009 Train_F1_batchwise: 0.77, Validation_accuracy: 64.37%, Validation_loss: 0.888659, Validation_F1_batchwise :0.64\n",
      "Epoch 46: Train_accuracy: 77.33%, Train_loss: 0.770314 Train_F1_batchwise: 0.77, Validation_accuracy: 63.97%, Validation_loss: 0.892569, Validation_F1_batchwise :0.64\n",
      "Epoch 47: Train_accuracy: 76.32%, Train_loss: 0.776792 Train_F1_batchwise: 0.76, Validation_accuracy: 65.18%, Validation_loss: 0.886414, Validation_F1_batchwise :0.65\n",
      "Epoch 48: Train_accuracy: 75.81%, Train_loss: 0.783214 Train_F1_batchwise: 0.76, Validation_accuracy: 65.59%, Validation_loss: 0.884304, Validation_F1_batchwise :0.65\n",
      "Epoch 49: Train_accuracy: 77.43%, Train_loss: 0.767448 Train_F1_batchwise: 0.77, Validation_accuracy: 62.75%, Validation_loss: 0.920947, Validation_F1_batchwise :0.63\n",
      "Early Stopping Cutoff!\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloaders\n",
    "train_dataloader = create_dataloader(batch_size=128, X=X_train, y=y_train)\n",
    "valid_dataloader = create_dataloader(batch_size=128, X=X_valid, y=y_valid)\n",
    "\n",
    "# Create the model\n",
    "model = Attention_Classifier()\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Cannot use Cross Entropy Loss with Softmax\n",
    "\n",
    "# Train the model\n",
    "train_accuracy, train_loss, train_f1, valid_accuracy, valid_loss, valid_f1 = train(train_dataloader=train_dataloader, validation_dataloader=valid_dataloader,\n",
    "                                                                                   model=model, loss_fn=loss_fn, optimizer=optimizer, epochs=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation F1 Score is: 0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "# Check final F1 Score\n",
    "final_valid_dataloader = create_dataloader(batch_size=len(X_valid), X=X_valid, y=y_valid)\n",
    "test_loss, test_correct, test_f1 = test_loop(final_valid_dataloader, model, loss_fn)\n",
    "print(f\"Final Validation F1 Score is: {test_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
